{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700645c3-7d5f-4937-b717-b22c529ddf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chrome WebDriver initialized successfully\n",
      "Starting data scraping...\n",
      "Loading page: https://asxenergy.com.au/futures_au\n",
      "✓ Page loaded successfully\n",
      "Found 30 tables on the page\n",
      "Warning: Error extracting table data: Message: no such element: Unable to locate element: {\"method\":\"tag name\",\"selector\":\"thead\"}\n",
      "  (Session info: chrome=137.0.7151.68); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff671a7fea5+79173]\n",
      "\tGetHandleVerifier [0x0x7ff671a7ff00+79264]\n",
      "\t(No symbol) [0x0x7ff671839e5a]\n",
      "\t(No symbol) [0x0x7ff671890586]\n",
      "\t(No symbol) [0x0x7ff67189083c]\n",
      "\t(No symbol) [0x0x7ff671882e4c]\n",
      "\t(No symbol) [0x0x7ff6718b89af]\n",
      "\t(No symbol) [0x0x7ff671882d16]\n",
      "\t(No symbol) [0x0x7ff6718b8b80]\n",
      "\t(No symbol) [0x0x7ff6718e100d]\n",
      "\t(No symbol) [0x0x7ff6718b8743]\n",
      "\t(No symbol) [0x0x7ff6718814c1]\n",
      "\t(No symbol) [0x0x7ff671882253]\n",
      "\tGetHandleVerifier [0x0x7ff671d4a2dd+3004797]\n",
      "\tGetHandleVerifier [0x0x7ff671d4472d+2981325]\n",
      "\tGetHandleVerifier [0x0x7ff671d63380+3107360]\n",
      "\tGetHandleVerifier [0x0x7ff671a9aa2e+188622]\n",
      "\tGetHandleVerifier [0x0x7ff671aa22bf+219487]\n",
      "\tGetHandleVerifier [0x0x7ff671a88df4+115860]\n",
      "\tGetHandleVerifier [0x0x7ff671a88fa9+116297]\n",
      "\tGetHandleVerifier [0x0x7ff671a6f558+11256]\n",
      "\tBaseThreadInitThunk [0x0x7ff91be9e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ff91d49c5dc+44]\n",
      "\n",
      "\n",
      "==================================================\n",
      "SCRAPING SUMMARY\n",
      "==================================================\n",
      "✓ WebDriver closed\n"
     ]
    }
   ],
   "source": [
    "# In[1]: Import Required Libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# In[2]: ASX Energy Futures Scraper Class\n",
    "class ASXEnergyFuturesScraper:\n",
    "    \"\"\"\n",
    "    A comprehensive scraper for ASX Energy futures data from https://asxenergy.com.au/futures_au\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless: bool = True, wait_timeout: int = 20):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with Chrome WebDriver\n",
    "        \n",
    "        Args:\n",
    "            headless (bool): Run browser in headless mode (no GUI)\n",
    "            wait_timeout (int): Maximum time to wait for elements to load\n",
    "        \"\"\"\n",
    "        self.url = \"https://asxenergy.com.au/futures_au\"\n",
    "        self.wait_timeout = wait_timeout\n",
    "        self.driver = None\n",
    "        self.setup_driver(headless)\n",
    "        \n",
    "    def setup_driver(self, headless: bool):\n",
    "        \"\"\"Setup Chrome WebDriver with optimal options\"\"\"\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize the Chrome driver\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            print(\"✓ Chrome WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error initializing WebDriver: {e}\")\n",
    "            print(\"Please ensure ChromeDriver is installed and in PATH\")\n",
    "            raise\n",
    "\n",
    "    def load_page(self) -> bool:\n",
    "        \"\"\"\n",
    "        Load the ASX Energy futures page and wait for data to load\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if page loaded successfully, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading page: {self.url}\")\n",
    "            self.driver.get(self.url)\n",
    "            \n",
    "            # Wait for the main content to load\n",
    "            wait = WebDriverWait(self.driver, self.wait_timeout)\n",
    "            \n",
    "            # Wait for tables to be present\n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "            \n",
    "            # Additional wait for dynamic content to fully load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            print(\"✓ Page loaded successfully\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading page: {e}\")\n",
    "            return False\n",
    "\n",
    "    def extract_table_data(self, table_element) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract data from a single table element\n",
    "        \n",
    "        Args:\n",
    "            table_element: Selenium WebElement representing a table\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of dictionaries containing row data\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        try:\n",
    "            # Find header row\n",
    "            header_row = table_element.find_element(By.TAG_NAME, \"thead\").find_element(By.TAG_NAME, \"tr\")\n",
    "            headers = [th.text.strip() for th in header_row.find_elements(By.TAG_NAME, \"th\")]\n",
    "            \n",
    "            # Find data rows\n",
    "            tbody = table_element.find_element(By.TAG_NAME, \"tbody\")\n",
    "            rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                if len(cells) == len(headers):\n",
    "                    row_data = {}\n",
    "                    for i, cell in enumerate(cells):\n",
    "                        row_data[headers[i]] = cell.text.strip()\n",
    "                    data.append(row_data)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error extracting table data: {e}\")\n",
    "            \n",
    "        return data\n",
    "\n",
    "    def scrape_all_futures_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Scrape all futures data from the page\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, pd.DataFrame]: Dictionary with section names as keys and DataFrames as values\n",
    "        \"\"\"\n",
    "        if not self.load_page():\n",
    "            return {}\n",
    "        \n",
    "        all_data = {}\n",
    "        \n",
    "        try:\n",
    "            # Find all sections/tables on the page\n",
    "            tables = self.driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            print(f\"Found {len(tables)} tables on the page\")\n",
    "            \n",
    "            # Common section names based on the image provided\n",
    "            section_names = [\n",
    "                \"Base_Month\", \"Base_Quarter\", \"Base_Strip\", \"Calendar\", \n",
    "                \"Caps_Strip\", \"Peak_Quarter\", \"Peak_Strip\"\n",
    "            ]\n",
    "            \n",
    "            for i, table in enumerate(tables):\n",
    "                try:\n",
    "                    # Try to find section title near the table\n",
    "                    section_name = f\"Table_{i+1}\"\n",
    "                    if i < len(section_names):\n",
    "                        section_name = section_names[i]\n",
    "                    \n",
    "                    # Extract data from the table\n",
    "                    table_data = self.extract_table_data(table)\n",
    "                    \n",
    "                    if table_data:\n",
    "                        df = pd.DataFrame(table_data)\n",
    "                        all_data[section_name] = df\n",
    "                        print(f\"✓ Extracted {len(table_data)} rows from {section_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing table {i+1}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return all_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error scraping data: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def get_specific_contracts(self, contract_types: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get data for specific contract types\n",
    "        \n",
    "        Args:\n",
    "            contract_types (List[str]): List of contract types to filter\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Combined DataFrame with specified contracts\n",
    "        \"\"\"\n",
    "        all_data = self.scrape_all_futures_data()\n",
    "        \n",
    "        if contract_types:\n",
    "            filtered_data = {}\n",
    "            for contract_type in contract_types:\n",
    "                if contract_type in all_data:\n",
    "                    filtered_data[contract_type] = all_data[contract_type]\n",
    "            all_data = filtered_data\n",
    "        \n",
    "        # Combine all data into a single DataFrame\n",
    "        combined_df = pd.DataFrame()\n",
    "        \n",
    "        for section_name, df in all_data.items():\n",
    "            df['Section'] = section_name\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "    def save_data(self, data: Dict[str, pd.DataFrame], output_format: str = 'excel'):\n",
    "        \"\"\"\n",
    "        Save scraped data to files\n",
    "        \n",
    "        Args:\n",
    "            data (Dict[str, pd.DataFrame]): Dictionary of DataFrames to save\n",
    "            output_format (str): 'excel', 'csv', or 'json'\n",
    "        \"\"\"\n",
    "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        if output_format.lower() == 'excel':\n",
    "            filename = f\"asx_energy_futures_{timestamp}.xlsx\"\n",
    "            with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "                for sheet_name, df in data.items():\n",
    "                    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            print(f\"✓ Data saved to {filename}\")\n",
    "            \n",
    "        elif output_format.lower() == 'csv':\n",
    "            for section_name, df in data.items():\n",
    "                filename = f\"asx_energy_{section_name}_{timestamp}.csv\"\n",
    "                df.to_csv(filename, index=False)\n",
    "                print(f\"✓ Data saved to {filename}\")\n",
    "                \n",
    "        elif output_format.lower() == 'json':\n",
    "            filename = f\"asx_energy_futures_{timestamp}.json\"\n",
    "            combined_data = {}\n",
    "            for section_name, df in data.items():\n",
    "                combined_data[section_name] = df.to_dict('records')\n",
    "            \n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(combined_data, f, indent=2)\n",
    "            print(f\"✓ Data saved to {filename}\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the WebDriver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            print(\"✓ WebDriver closed\")\n",
    "\n",
    "# In[3]: Usage Example - Basic Scraping\n",
    "def main_example():\n",
    "    \"\"\"Example usage of the ASX Energy Futures Scraper\"\"\"\n",
    "    \n",
    "    # Initialize the scraper\n",
    "    scraper = ASXEnergyFuturesScraper(headless=True)\n",
    "    \n",
    "    try:\n",
    "        # Scrape all futures data\n",
    "        print(\"Starting data scraping...\")\n",
    "        all_data = scraper.scrape_all_futures_data()\n",
    "        \n",
    "        # Display summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SCRAPING SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for section_name, df in all_data.items():\n",
    "            print(f\"{section_name}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "            if not df.empty:\n",
    "                print(f\"  Columns: {list(df.columns)}\")\n",
    "            print()\n",
    "        \n",
    "        # Save data in multiple formats\n",
    "        if all_data:\n",
    "            scraper.save_data(all_data, 'excel')\n",
    "            scraper.save_data(all_data, 'csv')\n",
    "            \n",
    "            # Display sample data from first section\n",
    "            if all_data:\n",
    "                first_section = list(all_data.keys())[0]\n",
    "                print(f\"Sample data from {first_section}:\")\n",
    "                print(all_data[first_section].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in main execution: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "# In[4]: Advanced Usage - Specific Contract Types\n",
    "def scrape_specific_contracts():\n",
    "    \"\"\"Example of scraping specific contract types\"\"\"\n",
    "    \n",
    "    scraper = ASXEnergyFuturesScraper(headless=True)\n",
    "    \n",
    "    try:\n",
    "        # Get data for specific contract types\n",
    "        contract_types = [\"Base_Month\", \"Peak_Quarter\", \"Base_Strip\"]\n",
    "        specific_data = scraper.get_specific_contracts(contract_types)\n",
    "        \n",
    "        print(\"Specific contracts data:\")\n",
    "        print(specific_data.head(10))\n",
    "        \n",
    "        # Save specific data\n",
    "        specific_data.to_csv(\"asx_specific_contracts.csv\", index=False)\n",
    "        print(\"✓ Specific contracts data saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "# In[5]: Real-time Monitoring Function\n",
    "def monitor_futures_data(interval_minutes: int = 15):\n",
    "    \"\"\"\n",
    "    Monitor futures data at regular intervals\n",
    "    \n",
    "    Args:\n",
    "        interval_minutes (int): Interval between scraping sessions in minutes\n",
    "    \"\"\"\n",
    "    import schedule\n",
    "    from datetime import datetime\n",
    "    \n",
    "    def scrape_and_save():\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"\\n[{timestamp}] Starting scheduled scraping...\")\n",
    "        \n",
    "        scraper = ASXEnergyFuturesScraper(headless=True)\n",
    "        try:\n",
    "            data = scraper.scrape_all_futures_data()\n",
    "            if data:\n",
    "                scraper.save_data(data, 'excel')\n",
    "                print(f\"[{timestamp}] ✓ Data scraped and saved successfully\")\n",
    "            else:\n",
    "                print(f\"[{timestamp}] ✗ No data retrieved\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{timestamp}] ✗ Error: {e}\")\n",
    "        finally:\n",
    "            scraper.close()\n",
    "    \n",
    "    # Schedule the scraping\n",
    "    schedule.every(interval_minutes).minutes.do(scrape_and_save)\n",
    "    \n",
    "    print(f\"Monitoring started - will scrape every {interval_minutes} minutes\")\n",
    "    print(\"Press Ctrl+C to stop monitoring\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)  # Check every minute\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nMonitoring stopped\")\n",
    "\n",
    "# In[6]: Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment the function you want to run:\n",
    "    \n",
    "    main_example()  # Basic scraping example\n",
    "    # scrape_specific_contracts()  # Specific contracts only\n",
    "    # monitor_futures_data(15)  # Real-time monitoring every 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dcf08-d7c1-4b49-bf5f-47d57a296314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
