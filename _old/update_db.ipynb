{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246c711b-bb40-4504-8c48-7a7aab09a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting ASX Futures Data Update Process\n",
      "==================================================\n",
      "‚úì Database connection established: C:/Users/ftelias/OneDrive/Documents/GitHub/humquote/futures_prices.db\n",
      "‚úì Database schema verified for table: futures_data\n",
      "üì° Scraping ASX futures data...\n",
      "‚úì Successfully scraped 3 records for 2025-06-06\n",
      "\n",
      "üìä Processing 3 new records...\n",
      "‚úì Loaded existing CSV: 1358 records\n",
      "‚úì CSV updated: 3 new records, 1361 total records\n",
      "‚úì Database connection established: C:/Users/ftelias/OneDrive/Documents/GitHub/humquote/futures_prices.db\n",
      "  ‚è≠Ô∏è  Skipped: 2025-06-06, Year 2026 (already exists)\n",
      "  ‚è≠Ô∏è  Skipped: 2025-06-06, Year 2027 (already exists)\n",
      "  ‚è≠Ô∏è  Skipped: 2025-06-06, Year 2028 (already exists)\n",
      "‚úì Database updated: 0 new records, 3 skipped\n",
      "‚úì Database connection established: C:/Users/ftelias/OneDrive/Documents/GitHub/humquote/futures_prices.db\n",
      "‚úì Database contains 1361 total records, sorted by Quote Date (descending)\n",
      "\n",
      "‚úÖ Data update process completed successfully!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Consolidated ASX Futures Data Updater\n",
    "=====================================\n",
    "This script scrapes daily ASX energy futures data and updates both a CSV file \n",
    "and SQLite database with the latest pricing information.\n",
    "\n",
    "Author: Generated for energy market data management\n",
    "Purpose: Automated daily data collection for Streamlit app\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import os\n",
    "from typing import Optional\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Configuration\n",
    "CSV_FILE_PATH = 'historical-futures-data.csv'\n",
    "DB_FILE_PATH = 'C:/Users/ftelias/OneDrive/Documents/GitHub/humquote/futures_prices.db'\n",
    "TABLE_NAME = 'futures_data'\n",
    "ASX_URL = 'https://www.asxenergy.com.au'\n",
    "\n",
    "def create_database_connection(db_file: str):\n",
    "    \"\"\"Create and return a connection to the SQLite database.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(f\"‚úì Database connection established: {db_file}\")\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"‚úó Error connecting to database: {e}\")\n",
    "        return None\n",
    "\n",
    "def setup_database_schema(db_file: str, table_name: str):\n",
    "    \"\"\"Create the database table if it doesn't exist.\"\"\"\n",
    "    conn = create_database_connection(db_file)\n",
    "    if conn is not None:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create table with wide format schema\n",
    "        cursor.execute(f'''\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                \"Quote Date\" TEXT,\n",
    "                \"Year\" INTEGER,\n",
    "                \"NSW\" REAL,\n",
    "                \"QLD\" REAL,\n",
    "                \"SA\" REAL,\n",
    "                \"VIC\" REAL,\n",
    "                PRIMARY KEY (\"Quote Date\", \"Year\")\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print(f\"‚úì Database schema verified for table: {table_name}\")\n",
    "    else:\n",
    "        print(\"‚úó Failed to setup database schema\")\n",
    "\n",
    "def scrape_asx_futures_data(url: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Scrape futures data from ASX Energy website and return in wide format.\n",
    "    \n",
    "    Args:\n",
    "        url: The ASX Energy website URL\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns: Quote Date, Year, NSW, QLD, SA, VIC\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üì° Scraping ASX futures data...\")\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the prices div\n",
    "        prices_div = soup.find('div', id='home-prices')\n",
    "        if not prices_div:\n",
    "            raise ValueError(\"Could not find prices div in the page\")\n",
    "            \n",
    "        # Extract date\n",
    "        date_cell = prices_div.find('td', style=\"color: #6c6c6c; font-size: 8pt; text-align: center;\")\n",
    "        if not date_cell:\n",
    "            raise ValueError(\"Could not find date cell in the page\")\n",
    "            \n",
    "        date_str = date_cell.get_text().strip()\n",
    "        quote_date = datetime.strptime(date_str, '%a %d %b %Y').date()\n",
    "        \n",
    "        # Extract prices table\n",
    "        prices_table = prices_div.find('table')\n",
    "        if not prices_table:\n",
    "            raise ValueError(\"Could not find prices table in the page\")\n",
    "        \n",
    "        # Parse table rows\n",
    "        rows = prices_table.find_all('tr')[1:]  # Skip header\n",
    "        data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            if not cells or len(cells) < 5:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                year = int(cells[0].get_text().strip())\n",
    "                nsw_price = float(cells[1].get_text().strip())\n",
    "                vic_price = float(cells[2].get_text().strip())\n",
    "                qld_price = float(cells[3].get_text().strip())\n",
    "                sa_price = float(cells[4].get_text().strip())\n",
    "                \n",
    "                row_data = {\n",
    "                    'Quote Date': quote_date,\n",
    "                    'Year': year,\n",
    "                    'NSW': round(nsw_price, 2),\n",
    "                    'QLD': round(qld_price, 2),\n",
    "                    'SA': round(sa_price, 2),\n",
    "                    'VIC': round(vic_price, 2)\n",
    "                }\n",
    "                data.append(row_data)\n",
    "                \n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"‚ö†Ô∏è  Warning: Skipping row due to parsing error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not data:\n",
    "            raise ValueError(\"No valid data rows found\")\n",
    "            \n",
    "        df = pd.DataFrame(data)\n",
    "        df['Quote Date'] = pd.to_datetime(df['Quote Date']).dt.date\n",
    "        \n",
    "        print(f\"‚úì Successfully scraped {len(df)} records for {quote_date}\")\n",
    "        return df\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚úó Error fetching data from website: {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"‚úó Error processing scraped data: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Unexpected error during scraping: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_existing_csv(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load existing CSV data or create empty DataFrame with correct schema.\"\"\"\n",
    "    if os.path.exists(csv_file):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df['Quote Date'] = pd.to_datetime(df['Quote Date']).dt.date\n",
    "            print(f\"‚úì Loaded existing CSV: {len(df)} records\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error loading CSV, creating new one: {e}\")\n",
    "    \n",
    "    # Create empty DataFrame with correct schema\n",
    "    print(\"üìÑ Creating new CSV file\")\n",
    "    return pd.DataFrame(columns=['Quote Date', 'Year', 'NSW', 'QLD', 'SA', 'VIC'])\n",
    "\n",
    "def update_csv_file(new_data: pd.DataFrame, csv_file: str):\n",
    "    \"\"\"Update CSV file with new data, avoiding duplicates.\"\"\"\n",
    "    try:\n",
    "        # Load existing data\n",
    "        existing_df = load_existing_csv(csv_file)\n",
    "        \n",
    "        # Combine and remove duplicates\n",
    "        if not existing_df.empty:\n",
    "            combined_df = pd.concat([existing_df, new_data], ignore_index=True)\n",
    "            # Remove duplicates based on Quote Date and Year\n",
    "            combined_df = combined_df.drop_duplicates(subset=['Quote Date', 'Year'], keep='last')\n",
    "        else:\n",
    "            combined_df = new_data.copy()\n",
    "        \n",
    "        # Sort by Quote Date descending\n",
    "        combined_df = combined_df.sort_values('Quote Date', ascending=False)\n",
    "        \n",
    "        # Convert Quote Date to string for CSV output\n",
    "        combined_df['Quote Date'] = pd.to_datetime(combined_df['Quote Date']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Save to CSV\n",
    "        combined_df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        new_records = len(new_data)\n",
    "        total_records = len(combined_df)\n",
    "        print(f\"‚úì CSV updated: {new_records} new records, {total_records} total records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error updating CSV file: {e}\")\n",
    "\n",
    "def update_database(new_data: pd.DataFrame, db_file: str, table_name: str):\n",
    "    \"\"\"Update database with new data, avoiding duplicates.\"\"\"\n",
    "    conn = create_database_connection(db_file)\n",
    "    if conn is None:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        new_records_count = 0\n",
    "        skipped_records_count = 0\n",
    "        \n",
    "        for _, row in new_data.iterrows():\n",
    "            quote_date_str = row['Quote Date'].strftime('%Y-%m-%d')\n",
    "            year = int(row['Year'])\n",
    "            \n",
    "            # Check if record already exists\n",
    "            cursor.execute(\n",
    "                f'SELECT COUNT(*) FROM {table_name} WHERE \"Quote Date\" = ? AND \"Year\" = ?',\n",
    "                (quote_date_str, year)\n",
    "            )\n",
    "            exists = cursor.fetchone()[0]\n",
    "            \n",
    "            if exists == 0:\n",
    "                # Insert new record\n",
    "                cursor.execute(f'''\n",
    "                    INSERT INTO {table_name} \n",
    "                    (\"Quote Date\", \"Year\", \"NSW\", \"QLD\", \"SA\", \"VIC\") \n",
    "                    VALUES (?, ?, ?, ?, ?, ?)\n",
    "                ''', (\n",
    "                    quote_date_str,\n",
    "                    year,\n",
    "                    float(row['NSW']),\n",
    "                    float(row['QLD']),\n",
    "                    float(row['SA']),\n",
    "                    float(row['VIC'])\n",
    "                ))\n",
    "                new_records_count += 1\n",
    "                print(f\"  ‚úì Added: {quote_date_str}, Year {year}\")\n",
    "            else:\n",
    "                skipped_records_count += 1\n",
    "                print(f\"  ‚è≠Ô∏è  Skipped: {quote_date_str}, Year {year} (already exists)\")\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"‚úì Database updated: {new_records_count} new records, {skipped_records_count} skipped\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error updating database: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def verify_data_sorting(db_file: str, table_name: str):\n",
    "    \"\"\"Verify that database data is sorted correctly and re-sort if needed.\"\"\"\n",
    "    conn = create_database_connection(db_file)\n",
    "    if conn is None:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Check if we need to re-sort the data\n",
    "        df = pd.read_sql_query(f'SELECT * FROM {table_name} ORDER BY \"Quote Date\" DESC', conn)\n",
    "        print(f\"‚úì Database contains {len(df)} total records, sorted by Quote Date (descending)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error verifying data sorting: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"üöÄ Starting ASX Futures Data Update Process\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Setup database schema\n",
    "    setup_database_schema(DB_FILE_PATH, TABLE_NAME)\n",
    "    \n",
    "    # Scrape new data\n",
    "    new_data = scrape_asx_futures_data(ASX_URL)\n",
    "    \n",
    "    if new_data is not None and not new_data.empty:\n",
    "        print(f\"\\nüìä Processing {len(new_data)} new records...\")\n",
    "        \n",
    "        # Update CSV file\n",
    "        update_csv_file(new_data, CSV_FILE_PATH)\n",
    "        \n",
    "        # Update database\n",
    "        update_database(new_data, DB_FILE_PATH, TABLE_NAME)\n",
    "        \n",
    "        # Verify sorting\n",
    "        verify_data_sorting(DB_FILE_PATH, TABLE_NAME)\n",
    "        \n",
    "        print(\"\\n‚úÖ Data update process completed successfully!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå No new data to process. Update process terminated.\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e5d99-9fd3-4d81-8809-91e819942d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
